{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e70e27",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-21T16:47:28.242845Z",
     "iopub.status.busy": "2025-03-21T16:47:28.242468Z",
     "iopub.status.idle": "2025-03-21T16:47:29.514020Z",
     "shell.execute_reply": "2025-03-21T16:47:29.513097Z"
    },
    "papermill": {
     "duration": 1.27817,
     "end_time": "2025-03-21T16:47:29.516032",
     "exception": false,
     "start_time": "2025-03-21T16:47:28.237862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e791fceb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T16:47:29.523411Z",
     "iopub.status.busy": "2025-03-21T16:47:29.523070Z",
     "iopub.status.idle": "2025-03-21T16:47:29.528376Z",
     "shell.execute_reply": "2025-03-21T16:47:29.527566Z"
    },
    "papermill": {
     "duration": 0.010425,
     "end_time": "2025-03-21T16:47:29.529998",
     "exception": false,
     "start_time": "2025-03-21T16:47:29.519573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision.transforms as transforms\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# # Custom Dataset Class\n",
    "# class ParticleDataset(Dataset):\n",
    "#     def __init__(self, hdf5_files, transform=None):\n",
    "#         self.data = []\n",
    "#         self.labels = []\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         for idx, file in enumerate(hdf5_files):\n",
    "#             with h5py.File(file, 'r') as f:\n",
    "#                 X = f['X'][:]\n",
    "#                 y = np.full((X.shape[0],), idx)  # Assign label 0 for electrons, 1 for photons\n",
    "#                 self.data.append(X)\n",
    "#                 self.labels.append(y)\n",
    "        \n",
    "#         self.data = np.concatenate(self.data, axis=0)\n",
    "#         self.labels = np.concatenate(self.labels, axis=0)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         img = self.data[idx]\n",
    "#         label = self.labels[idx]\n",
    "        \n",
    "#         if self.transform:\n",
    "#             img = self.transform(img)\n",
    "        \n",
    "#         return img, label\n",
    "\n",
    "# # Load Dataset\n",
    "# full_dataset = ParticleDataset(['/kaggle/input/cms-particle-collison/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5', \n",
    "#                                 '/kaggle/input/cms-particle-collison/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5'])\n",
    "\n",
    "# # Split into Train and Test  # **MODIFIED**\n",
    "# total_size = len(full_dataset)\n",
    "# train_size = int(0.8 * total_size)\n",
    "# test_size = total_size - train_size\n",
    "# train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# # Compute Mean and Std for Normalization on Train Set  # **MODIFIED**\n",
    "# def compute_mean_std(dataset):\n",
    "#     loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "#     mean = 0.0\n",
    "#     std = 0.0\n",
    "#     total_samples = 0\n",
    "#     for images, _ in loader:\n",
    "#         batch_samples = images.size(0)  # Batch size\n",
    "#         images = images.view(batch_samples, -1)\n",
    "#         mean += images.mean(dim=1).sum().item()\n",
    "#         std += images.std(dim=1).sum().item()\n",
    "#         total_samples += batch_samples\n",
    "#     mean /= total_samples\n",
    "#     std /= total_samples\n",
    "#     return mean, std\n",
    "\n",
    "# mean, std = compute_mean_std(train_dataset)  # **MODIFIED**\n",
    "# print(f\"Computed Mean: {mean}, Std: {std}\")\n",
    "\n",
    "# # Define Data Augmentation and Normalization  # **MODIFIED**\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(15),\n",
    "#     transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.Normalize((mean,), (std,))\n",
    "# ])\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((mean,), (std,))\n",
    "# ])\n",
    "\n",
    "# # Reload dataset with transforms\n",
    "# train_dataset.dataset.transform = train_transform\n",
    "# test_dataset.dataset.transform = test_transform\n",
    "\n",
    "# # Data Loaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# print(\"Data Augmented\")\n",
    "\n",
    "# # Define ResNet-like Model\n",
    "# class ResNet15(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ResNet15, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(2, 64, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.res_layers = self._make_res_layers(64, 128, 4)\n",
    "#         # Compute final feature map size dynamically\n",
    "#         with torch.no_grad():\n",
    "#             dummy_input = torch.randn(1, 2, 32, 32)\n",
    "#             dummy_output = self.res_layers(self.relu(self.bn1(self.conv1(dummy_input))))\n",
    "#             final_h, final_w = dummy_output.shape[2], dummy_output.shape[3]\n",
    "        \n",
    "#         self.fc = nn.Linear(128 * final_h * final_w, 2)\n",
    "    \n",
    "#     def _make_res_layers(self, in_channels, out_channels, blocks):\n",
    "#         layers = []\n",
    "#         for _ in range(blocks):\n",
    "#             layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "#             layers.append(nn.BatchNorm2d(out_channels))\n",
    "#             layers.append(nn.ReLU(inplace=True))\n",
    "#             in_channels = out_channels\n",
    "#         return nn.Sequential(*layers)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.res_layers(x)\n",
    "#         x = torch.flatten(x, start_dim=1)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# # Training Setup\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = ResNet15().to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# print(\"Model Created\")\n",
    "\n",
    "# # Training Loop\n",
    "# num_epochs = 30\n",
    "# train_losses = []\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     scheduler.step()\n",
    "#     epoch_loss = running_loss / len(train_loader)\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# print(\"Training Finished\")\n",
    "\n",
    "# # Plot Training Loss\n",
    "# plt.figure()\n",
    "# plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss Over Epochs')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Evaluation\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in test_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "# torch.save(model.state_dict(), \"resnet15_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32618da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T16:47:29.536238Z",
     "iopub.status.busy": "2025-03-21T16:47:29.536035Z",
     "iopub.status.idle": "2025-03-21T18:46:31.592871Z",
     "shell.execute_reply": "2025-03-21T18:46:31.591818Z"
    },
    "papermill": {
     "duration": 7142.061608,
     "end_time": "2025-03-21T18:46:31.594444",
     "exception": false,
     "start_time": "2025-03-21T16:47:29.532836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Mean: 0.00047782219999178955, Std: 0.04881534787037047\n",
      "Data Augmented\n",
      "Model Created\n",
      "Epoch 1, Loss: 0.0000\n",
      "Epoch 2, Loss: 0.0000\n",
      "Epoch 3, Loss: 0.0000\n",
      "Epoch 4, Loss: 0.0000\n",
      "Epoch 5, Loss: 0.0000\n",
      "Epoch 6, Loss: 0.0000\n",
      "Epoch 7, Loss: 0.0000\n",
      "Epoch 8, Loss: 0.0000\n",
      "Epoch 9, Loss: 0.0000\n",
      "Epoch 10, Loss: 0.0000\n",
      "Epoch 11, Loss: 0.0000\n",
      "Epoch 12, Loss: 0.0000\n",
      "Epoch 13, Loss: 0.0000\n",
      "Epoch 14, Loss: 0.0000\n",
      "Epoch 15, Loss: 0.0000\n",
      "Epoch 16, Loss: 0.0000\n",
      "Epoch 17, Loss: 0.0000\n",
      "Epoch 18, Loss: 0.0000\n",
      "Epoch 19, Loss: 0.0000\n",
      "Epoch 20, Loss: 0.0000\n",
      "Epoch 21, Loss: 0.0000\n",
      "Epoch 22, Loss: 0.0000\n",
      "Epoch 23, Loss: 0.0000\n",
      "Epoch 24, Loss: 0.0000\n",
      "Epoch 25, Loss: 0.0000\n",
      "Epoch 26, Loss: 0.0000\n",
      "Epoch 27, Loss: 0.0000\n",
      "Epoch 28, Loss: 0.0000\n",
      "Epoch 29, Loss: 0.0000\n",
      "Epoch 30, Loss: 0.0000\n",
      "Training Finished\n",
      "Accuracy: 71.54%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ParticleDataset(Dataset):\n",
    "    def __init__(self, hdf5_files, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        for idx, file in enumerate(hdf5_files):\n",
    "            with h5py.File(file, 'r') as f:\n",
    "                X = f['X'][:]\n",
    "                y = np.full((X.shape[0],), idx)  # Assign label 0 for electrons, 1 for photons\n",
    "                self.data.append(X)\n",
    "                self.labels.append(y)\n",
    "        \n",
    "        self.data = np.concatenate(self.data, axis=0)\n",
    "        self.labels = np.concatenate(self.labels, axis=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "# Load Dataset\n",
    "full_dataset = ParticleDataset(['/kaggle/input/cms-particle-collison/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5', \n",
    "                                '/kaggle/input/cms-particle-collison/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5'])\n",
    "\n",
    "# Split into Train and Test\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Compute Mean and Std for Normalization on Train Set\n",
    "def compute_mean_std(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_samples = 0\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)  # Batch size\n",
    "        images = images.view(batch_samples, -1)\n",
    "        mean += images.mean(dim=1).sum().item()\n",
    "        std += images.std(dim=1).sum().item()\n",
    "        total_samples += batch_samples\n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "    return mean, std\n",
    "\n",
    "mean, std = compute_mean_std(train_dataset)\n",
    "print(f\"Computed Mean: {mean}, Std: {std}\")\n",
    "\n",
    "# Define Data Augmentation and Normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "    transforms.Normalize((mean,), (std,))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean,), (std,))\n",
    "])\n",
    "\n",
    "# # Reload dataset with transforms\n",
    "train_dataset.dataset.transform = train_transform\n",
    "test_dataset.dataset.transform = test_transform\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Data Augmented\")\n",
    "\n",
    "# Define Improved ResNet-15 Model\n",
    "class ResNet15Enhanced(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet15Enhanced, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.res_layers = self._make_res_layers(64, 128, 4)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(128, 2)\n",
    "    \n",
    "    def _make_res_layers(self, in_channels, out_channels, blocks):\n",
    "        layers = []\n",
    "        for _ in range(blocks):\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.res_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Training Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNet15Enhanced().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"Model Created\")\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "print(\"Training Finished\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "torch.save(model.state_dict(), \"resnet15_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ccc10",
   "metadata": {
    "papermill": {
     "duration": 0.003181,
     "end_time": "2025-03-21T18:46:31.601495",
     "exception": false,
     "start_time": "2025-03-21T18:46:31.598314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6844145,
     "sourceId": 10995179,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7149.931693,
   "end_time": "2025-03-21T18:46:34.752139",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-21T16:47:24.820446",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
